---
title: "simple_logistic"
author: "610 group"
date: "November 24, 2018"
output: html_document
---

Setting up
```{r,message = FALSE, warning = FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(caret)
```


This heart data set contains a binary variable called num that indicates whether a person is at risk for heart disease or not. Specifically, 0 means < 50% diameter narrowing and  1 means > 50% diameter narrowing. We will also be working with trestbps, which is a persons resting blood pressure. We will not be cenetering trestbps as we do not care about the intercept.
```{r}
heart = read.csv("C:/Users/mitch/Documents/School/610_project/heart.csv") 
heart$num = ifelse(heart$num=="No.Risk", 0,1) 
heart = heart %>% select(num, trestbps)
ggplot(heart, aes(x = trestbps, y = num )) + geom_point() 
```


Then we make a subset of our data and create a model. 2/3 the data will make the model and 1/3 will be saved for testing. We are using only one variable to keep it simple. It looks like trestbps is significant. To interpet, e^(.017) = 1.017 is the increase in the odds of being at risk of heart disease as blood pressure increases by 1. If you convert that to probability we have that for each 1 unit increase in blood pressure, you have a .017 increase in probability of being at risk of heart disease. Now we will assume that your model is what you want it to be and go on with how to use it. 
```{r}
set.seed(1)
hearttrain = sample(303,303*(2/3))
model = glm(num~trestbps , data = heart,family="binomial", subset = hearttrain)
summary(model)
exp(.017)
```


This s curve is to show how our model makes predictions. It struggles to make predictions around the mean but it is confident otherwise. 
```{r}
df = data.frame(tbps = heart[hearttrain,]$trestbps, risk = heart[hearttrain,]$num, fit = model$fitted.values)
ggplot(df) + geom_line(aes(x = tbps, y = fit)) +  geom_point(aes(x= tbps, y = risk)) 
```


Now we make predictions for our real data that we didn't use to make the model.
```{r}
predicts = predict(model,heart[-hearttrain,], type = "response")
data = data.frame(heart[-hearttrain,],predicts)
head(data)
```


We need to find a cutoff for what probabilities will mean not at risk and which will mean at risk. This is done by maximizing sensitivity and specificity. We do this by looking at a graph of possible sensitivities and specificities. We choose the x value where they intersect. This maximizes sensitivity and specificity. We wont be biased towards positives or negatives. We'll go with .44.
```{r}
predictions <- prediction(predicts,heart[-hearttrain,]$num)

plot(unlist(performance(predictions, "sens")@x.values), unlist(performance(predictions, "sens")@y.values), 
     type="l", lwd=2, ylab="Specificity", xlab="Cutoff")
par(new=TRUE)
plot(unlist(performance(predictions, "spec")@x.values), unlist(performance(predictions, "spec")@y.values), 
     type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2))
mtext("Specificity",side=4, padj=-2, col='red')

```


Here we put the predictions and truths into one dataframe. 
```{r}
plot.mpp <- data.frame(prediction = predicts, truth = factor(heart[-hearttrain,]$num, labels=c("norisk", "risk")))
```


This plots the probabilites and the cutoff line to visualize how good our predictions are. We have clear groupings which is great. 
```{r}
ggplot(plot.mpp, aes(x=truth, y=prediction, fill=truth)) + geom_jitter(width=.2) + geom_violin(alpha=.4) + theme_bw() + geom_hline(yintercept=.44)
```


Heres the sensitivity(true positive) and the specificity(true negative). 
```{r}
plot.mpp$pred.class <- ifelse(plot.mpp$prediction <.44, 0,1) 
plot.mpp$pred.class <- factor(plot.mpp$pred.class, labels=c("norisk", "risk"))
confusionMatrix(plot.mpp$pred.class, plot.mpp$truth, positive="risk")
```



